# -*- coding: utf-8 -*-
"""Kelompok 7_Final Project Deep Learning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WiBt8A1Uy-fhVrer6olvOe4sNBN4Msg1

#1.Load and Transform Data

###1.1 Mount Google Drive
"""

try:
    from google.colab import drive
    drive.mount('/content/drive', force_remount=True)
except:
    print("Drive mount failed. Restart runtime then try again.")

"""###1.2 Setup Path File dan Extract Folder"""

import os

# Folder tempat menyimpan file
DRIVE_BASE = '/content/drive/MyDrive/DATA PJBL DEEL B'

# File ZIP gambar
DRIVE_ZIP_PATH = os.path.join(DRIVE_BASE, 'img.zip')

# Folder anotasi
ANNOT_DIR = DRIVE_BASE

# Lokasi extract di storage Colab
EXTRACT_TO = '/content/deepfashion_img'

print("DRIVE_ZIP_PATH:", DRIVE_ZIP_PATH)
print("ANNOT_DIR:", ANNOT_DIR)
print("EXTRACT_TO:", EXTRACT_TO)

"""###1.3 Extract ZIP"""

import zipfile

if not os.path.exists(EXTRACT_TO):
    print("Extracting img.zip")
    with zipfile.ZipFile(DRIVE_ZIP_PATH, 'r') as z:
        z.extractall(EXTRACT_TO)
    print("Extraction selesai!")
else:
    print("Folder gambar sudah ada, skip extract.")

"""#2.Data Preparation

###2.1 Kumpulkan Semua Gambar dan Label
"""

import pandas as pd
from pathlib import Path
from sklearn.model_selection import train_test_split

# Lokasi gambar setelah extract
IMG_ROOT = EXTRACT_TO + "/img"

image_paths = []
labels = []

for root, dirs, files in os.walk(IMG_ROOT):
    for f in files:
        if f.lower().endswith(('.jpg', '.jpeg', '.png')):
            full_path = os.path.join(root, f)
            folder = Path(full_path).parts[-2]
            image_paths.append(full_path)
            labels.append(folder)

df = pd.DataFrame({
    'path': image_paths,
    'label_name': labels
})

print("Total images awal      :", len(df))
print("Total categories awal  :", df['label_name'].nunique())
df

"""###2.2 Filter Kategori"""

# Menghitung jumlah gambar per kategori
counts = df['label_name'].value_counts()

# Menyaring kategori yang memiliki lebih dari atau sama dengan 130 gambar
valid_categories = counts[counts >= 130].index

# Filter dataframe hanya untuk kategori yang valid
df_filtered = df[df['label_name'].isin(valid_categories)].reset_index(drop=True)

# Menampilkan hasil
print("Total images setelah filter:", len(df_filtered))
print("Total kategori        :", df_filtered['label_name'].nunique())

# Jika ingin menggunakan df_filtered untuk proses selanjutnya
df = df_filtered

"""###2.3 Encode Label Jadi Angka"""

label_to_idx = {name: i for i, name in enumerate(sorted(df['label_name'].unique()))}
df['label'] = df['label_name'].map(label_to_idx)
df

"""###2.4 Pembagian Data (Train, Validation, Test)"""

# Data Train
train_df, temp_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df['label']
)

# Data Val/Test
val_df, test_df = train_test_split(
    temp_df,
    test_size=0.5,
    random_state=42,
    shuffle=True
)

print("UKURAN DATASET:")
print("Train  :", len(train_df))
print("Val    :", len(val_df))
print("Test   :", len(test_df))

"""###2.5 Simpan dataset ke CSV"""

train_df.to_csv(f"{EXTRACT_TO}/train_manifest.csv", index=False)
val_df.to_csv(f"{EXTRACT_TO}/val_manifest.csv", index=False)
test_df.to_csv(f"{EXTRACT_TO}/test_manifest.csv", index=False)

print("\nManifest files saved.")

"""###2.6 Data Augmentation"""

import tensorflow as tf

IMG_SIZE = 224
BATCH_SIZE = 32
AUTOTUNE = tf.data.AUTOTUNE

# EfficientNet preprocessor
preprocess = tf.keras.applications.efficientnet.preprocess_input

#Function load & preprocess image
def load_image(path, label):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))
    img = preprocess(img)
    return img, label

#Augmentasi untuk training
data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.05),
    tf.keras.layers.RandomZoom(0.1),
])

def load_image_train(path, label):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))
    img = data_augmentation(img)
    img = preprocess(img)
    return img, label

"""###2.7 Dataset Pipeline"""

#DataFrame â†’ tf.data
def df_to_dataset(df, training=False):
    paths = df['path'].values
    labels = df['label'].values

    ds = tf.data.Dataset.from_tensor_slices((paths, labels))

    if training:
        ds = ds.shuffle(buffer_size=len(df))
        ds = ds.map(load_image_train, num_parallel_calls=AUTOTUNE)
    else:
        ds = ds.map(load_image, num_parallel_calls=AUTOTUNE)

    ds = ds.batch(BATCH_SIZE)
    ds = ds.prefetch(AUTOTUNE)
    return ds

train_ds = df_to_dataset(train_df, training=True)
val_ds   = df_to_dataset(val_df, training=False)
test_ds  = df_to_dataset(test_df, training=False)

print("Train batches:", len(train_ds))
print("Val batches:", len(val_ds))
print("Test batches:", len(test_ds))

"""###2.8 Visualization image dari dataset"""

import matplotlib.pyplot as plt

batch_images, batch_labels = next(iter(train_ds))

plt.figure(figsize=(12, 8))
for i in range(9):
    plt.subplot(3, 3, i+1)
    plt.imshow(batch_images[i].numpy().astype("uint8"))
    plt.title(f"Label: {batch_labels[i].numpy()}")
    plt.axis("off")
plt.show()

"""#3.Model Building

## 3.1 Baseline: Fine-grained Classification (64 Classes)

###a. Memuat model EfficientNetB0
"""

import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.optimizers import Adam

# Jumlah kelas dan ukuran gambar
IMG_SIZE = 224
NUM_CLASSES = df['label'].nunique()  # = 64
BATCH_SIZE = 32

# Load model pre-trained EfficientNetB0
pretrained_model = tf.keras.applications.EfficientNetB0(
    input_shape=(IMG_SIZE, IMG_SIZE, 3),
    include_top=False,
    weights='imagenet',
    pooling='max'
)

# Freeze base model
pretrained_model.trainable = False

"""###b. Membuat Model Kustom untuk Klasifikasi"""

# Membuat layer custom untuk klasifikasi
inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
x = pretrained_model(inputs, training=False)  # Freeze base model
x = layers.Dense(128, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.45)(x)
x = layers.Dense(256, activation='relu')(x)
x = layers.BatchNormalization()(x)
x = layers.Dropout(0.45)(x)

# Output layer untuk klasifikasi
outputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)

# Model akhir
model = models.Model(inputs=inputs, outputs=outputs)

"""###c. Kompilasi Model"""

from tensorflow.keras.optimizers import Adam
model.compile(
    optimizer=Adam(1e-4),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

model.summary()

"""###d. Membuat Callback untuk Model Checkpoint dan EarlyStopping"""

from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau

# Membuat callback untuk menyimpan model terbaik
checkpoint_path = "indo_fashion_classification_model_checkpoint.weights.h5"
checkpoint_callback = ModelCheckpoint(checkpoint_path,
                                      save_weights_only=True,
                                      monitor="val_accuracy",
                                      save_best_only=True)

# Early stopping jika validasi loss tidak membaik
early_stopping = EarlyStopping(monitor="val_loss", patience=5, restore_best_weights=True)

# Mengurangi learning rate jika validasi loss tidak membaik
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=1e-6)

"""###e. Latih Model"""

# Menyiapkan dataset
train_ds = df_to_dataset(train_df, training=True)
val_ds = df_to_dataset(val_df, training=False)

# Melatih model
history = model.fit(
    train_ds,
    steps_per_epoch=len(train_ds),
    validation_data=val_ds,
    validation_steps=len(val_ds),
    epochs=20,
    callbacks=[
        early_stopping,
        checkpoint_callback,
        reduce_lr
    ]
)

# Menyimpan model
model.save("fashion_FineGrainedClassification_model.keras")

"""##3.2 Proposed Method: Coarse-grained Classification (4 Classes)

####a. Pre-processing Coarse-grained Classification
"""

# Coarse-Grained Label Construction
def simplify_label(name):
    name = name.lower()

    if "blouse" in name:
        return "Blouse"
    elif "blazer" in name:
        return "Blazer"
    elif "dress" in name:
        return "Dress"
    elif "sweater" in name or "knit" in name:
        return "Sweater"
    elif "bomber" in name or "jacket" in name:
        return "Jacket"
    elif "anorak" in name or "coat" in name:
        return "Coat"
    elif "pants" in name or "jeans" in name or "skirt" in name:
        return "Bottom"
    else:
        return "Other"

# Coarse-Grained Label Mapping (CG)

# Membuat label coarse-grained (CG)
df["cg_label_name"] = df["label_name"].apply(simplify_label)

# Cek distribusi label CG
print(df["cg_label_name"].value_counts())

# Encoding Coarse-Grained Labels

# Mapping label CG ke indeks numerik
cg_label_map = {
    name: i for i, name in enumerate(sorted(df["cg_label_name"].unique()))
}

# Encode label
df["cg_label"] = df["cg_label_name"].map(cg_label_map)

# Jumlah kelas CG
CG_NUM_CLASSES = len(cg_label_map)
print("CG_NUM_CLASSES:", CG_NUM_CLASSES)

# Train / Validation / Test Split (Coarse-Grained)
from sklearn.model_selection import train_test_split

# Data Train (CG)
cg_train_df, cg_temp_df = train_test_split(
    df,
    test_size=0.2,
    random_state=42,
    stratify=df["cg_label"]
)

# Data Validation & Test (CG)
cg_val_df, cg_test_df = train_test_split(
    cg_temp_df,
    test_size=0.5,
    random_state=42,
    stratify=cg_temp_df["cg_label"]
)

print("UKURAN DATASET COARSE-GRAINED:")
print("Train :", len(cg_train_df))
print("Val   :", len(cg_val_df))
print("Test  :", len(cg_test_df))

# Image Preprocessing & Data Augmentation (Coarse-Grained)
import tensorflow as tf

CG_IMG_SIZE = 224
CG_BATCH_SIZE = 32
CG_AUTOTUNE = tf.data.AUTOTUNE

# EfficientNet preprocessor (CG)
cg_preprocess = tf.keras.applications.efficientnet.preprocess_input

# Load & preprocess image (validation / test)
def cg_load_image(path, label):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (CG_IMG_SIZE, CG_IMG_SIZE))
    img = cg_preprocess(img)
    return img, label

# Data augmentation (training only)
cg_data_augmentation = tf.keras.Sequential([
    tf.keras.layers.RandomFlip("horizontal"),
    tf.keras.layers.RandomRotation(0.05),
    tf.keras.layers.RandomZoom(0.1),
])

# Load, augment, & preprocess image (training)
def cg_load_image_train(path, label):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (CG_IMG_SIZE, CG_IMG_SIZE))
    img = cg_data_augmentation(img)
    img = cg_preprocess(img)
    return img, label

# DataFrame â†’ tf.data Pipeline Construction (Coarse-Grained)
def cg_df_to_dataset(df, training=False):
    paths = df["path"].values
    labels = df["cg_label"].values

    ds = tf.data.Dataset.from_tensor_slices((paths, labels))

    if training:
        ds = ds.shuffle(buffer_size=len(df))
        ds = ds.map(cg_load_image_train, num_parallel_calls=CG_AUTOTUNE)
    else:
        ds = ds.map(cg_load_image, num_parallel_calls=CG_AUTOTUNE)

    ds = ds.batch(CG_BATCH_SIZE)
    ds = ds.prefetch(CG_AUTOTUNE)
    return ds

cg_train_ds = cg_df_to_dataset(cg_train_df, training=True)
cg_val_ds   = cg_df_to_dataset(cg_val_df, training=False)
cg_test_ds  = cg_df_to_dataset(cg_test_df, training=False)

print("CG Train batches:", len(cg_train_ds))
print("CG Val batches  :", len(cg_val_ds))
print("CG Test batches :", len(cg_test_ds))

"""###b. Memuat model EfficientNetB0 Coarse-Grained"""

CG_NUM_CLASSES = 4

cg_base_model = tf.keras.applications.EfficientNetB0(
    include_top=False,
    weights="imagenet",
    input_shape=(CG_IMG_SIZE, CG_IMG_SIZE, 3),
    pooling="avg"
)

# Freeze base model
cg_base_model.trainable = False

"""###c. Membuat Model Kustom untuk Coarse-Grained Classification"""

cg_inputs = tf.keras.Input(shape=(CG_IMG_SIZE, CG_IMG_SIZE, 3))
x = cg_base_model(cg_inputs, training=False)
x = tf.keras.layers.Dense(256, activation="relu")(x)
x = tf.keras.layers.BatchNormalization()(x)
x = tf.keras.layers.Dropout(0.5)(x)
cg_outputs = tf.keras.layers.Dense(CG_NUM_CLASSES, activation="softmax")(x)

cg_model = tf.keras.Model(cg_inputs, cg_outputs)

"""###d. Kompilasi Model"""

from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.optimizers import Adam

# Callback (EarlyStopping)
cg_early_stopping = EarlyStopping(
    monitor="val_loss",
    patience=5,
    restore_best_weights=True
)

# Compile model
cg_model.compile(
    optimizer=Adam(learning_rate=1e-3),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)
cg_model.summary()

"""###e. Latih Model"""

cg_history = cg_model.fit(
    cg_train_ds,
    validation_data=cg_val_ds,
    epochs=20,
    callbacks=[cg_early_stopping]
)

# Menyimpan model
cg_model.save("fashion_CoarceGrainedClassification_model.keras")

"""#4.Model Evaluation

###4.1 Fine-grained Classification (64 Class)

####a. Model Evaluation
"""

test_ds = df_to_dataset(test_df, training=False)

test_loss, test_acc = model.evaluate(test_ds, steps=len(test_ds))
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_acc}")

"""####b. Loss Curves Visualization"""

# Visualisasi kurva loss
plt.figure(figsize=(12, 6))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(history.history['loss'], label='Loss (train)')
plt.plot(history.history['val_loss'], label='Loss (val)')
plt.title('Loss Curve')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(history.history['accuracy'], label='Accuracy (train)')
plt.plot(history.history['val_accuracy'], label='Accuracy (val)')
plt.title('Accuracy Curve')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

# Menampilkan plot
plt.tight_layout()
plt.show()

"""###4.2 Coarse-grained Classification (4 Class)

####a. Model Evaluation
"""

# Evaluasi model coarse-grained
cg_test_ds = cg_df_to_dataset(cg_test_df, training=False)

cg_test_loss, cg_test_acc = cg_model.evaluate(
    cg_test_ds,
    steps=len(cg_test_ds)
)

print(f"CG Test Loss     : {cg_test_loss:.4f}")
print(f"CG Test Accuracy : {cg_test_acc:.4f}")

"""####b. Loss Curves Visualization"""

import matplotlib.pyplot as plt

cg_history_plot = cg_history  # history dari cg_model.fit()

plt.figure(figsize=(12, 6))

# Plot Loss
plt.subplot(1, 2, 1)
plt.plot(cg_history_plot.history["loss"], label="Loss (train)")
plt.plot(cg_history_plot.history["val_loss"], label="Loss (val)")
plt.title("Coarse-Grained Loss Curve")
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.legend()

# Plot Accuracy
plt.subplot(1, 2, 2)
plt.plot(cg_history_plot.history["accuracy"], label="Accuracy (train)")
plt.plot(cg_history_plot.history["val_accuracy"], label="Accuracy (val)")
plt.title("Coarse-Grained Accuracy Curve")
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.legend()

plt.tight_layout()
plt.show()

"""# 5. Prediction Data test

###5.1 Fine-grained Classification (64 Class)

####a. Classification Report
"""

from sklearn.metrics import classification_report, confusion_matrix
import numpy as np

# Mengambil prediksi untuk data uji
y_true = []
y_pred = []

for images, labels in test_ds:
    y_true.append(labels.numpy())
    y_pred.append(model.predict(images))

# Mengubah prediksi dan labels ke bentuk yang sesuai
y_true = np.concatenate(y_true)
y_pred = np.concatenate(y_pred)
y_pred = np.argmax(y_pred, axis=-1)  # Convert logits to class labels

# Menghasilkan classification report
print("=== Fine-Grained Classification Report ===")
print(classification_report(y_true, y_pred, target_names=df['label_name'].unique()))

def load_image_raw(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))
    img = tf.cast(img, tf.float32) / 255.0   # cukup normal [0,1]
    return img

sample_df = test_df.sample(9, random_state=42)

images = []
labels = []

for _, row in sample_df.iterrows():
    img = load_image_raw(row['path'])
    images.append(img)
    labels.append(row['label'])

images = tf.stack(images)
labels = np.array(labels)

images_model = tf.keras.applications.efficientnet.preprocess_input(images * 255)

preds = model.predict(images_model)
preds = np.argmax(preds, axis=1)

plt.figure(figsize=(10,10))
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(images[i].numpy())
    plt.title(
        f"True: {df['label_name'].unique()[labels[i]]}\n"
        f"Pred: {df['label_name'].unique()[preds[i]]}"
    )
    plt.axis("off")

plt.tight_layout()
plt.show()

"""####b. Confusion Matrix"""

import numpy as np

# Ambil true labels
y_true = []
for _, labels in test_ds:
    y_true.extend(labels.numpy())
y_true = np.array(y_true)

# Prediksi model
y_pred_prob = model.predict(test_ds)
y_pred = np.argmax(y_pred_prob, axis=1)
cm = confusion_matrix(y_true, y_pred)

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(12, 10))
sns.heatmap(
    cm,
    cmap="Blues",
    xticklabels=range(NUM_CLASSES),
    yticklabels=range(NUM_CLASSES),
    cbar=True
)

plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix â€“ Fashion Classification")
plt.tight_layout()
plt.show()

"""###5.2 Coarse-grained Classification (4 Class)"""

import numpy as np
from sklearn.metrics import classification_report, confusion_matrix

# Ambil label asli & prediksi
y_true = []
y_pred = []

for images, labels in cg_test_ds:
    preds = cg_model.predict(images, verbose=0)
    y_true.extend(labels.numpy())
    y_pred.extend(np.argmax(preds, axis=1))

y_true = np.array(y_true)
y_pred = np.array(y_pred)

"""####a. Classification Report"""

# Ambil nama kelas
cg_class_names = list(cg_label_map.keys())

print("=== Coarse-Grained Classification Report ===")
print(classification_report(
    y_true,
    y_pred,
    target_names=cg_class_names,
    digits=4
))

for d in [train_df, val_df, test_df]:
    d["cg_label_name"] = d["label_name"].apply(simplify_label)
    d["cg_label"] = d["cg_label_name"].map(cg_label_map)

cg_class_names = [
    name for name, idx in sorted(cg_label_map.items(), key=lambda x: x[1])
]
print(cg_class_names)

import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf

# Ambil 9 sampel dari test set
sample_df = test_df.sample(9, random_state=42)

images = []
labels = []

for _, row in sample_df.iterrows():
    img = load_image_raw(row["path"])   # gambar ASLI (tanpa preprocess)
    images.append(img)
    labels.append(row["cg_label"])      # ðŸ”´ PENTING: coarse label

images = tf.stack(images)
labels = np.array(labels)

# Preprocess untuk model
images_model = tf.keras.applications.efficientnet.preprocess_input(images * 255)

# Prediksi
preds = cg_model.predict(images_model, verbose=0)
preds = np.argmax(preds, axis=1)

# Plot
plt.figure(figsize=(10,10))
for i in range(9):
    plt.subplot(3,3,i+1)
    plt.imshow(images[i].numpy())
    plt.title(
        f"True: {cg_class_names[labels[i]]}\n"
        f"Pred: {cg_class_names[preds[i]]}"
    )
    plt.axis("off")

plt.tight_layout()
plt.show()

"""####b. Confusion Matrix"""

import matplotlib.pyplot as plt
import seaborn as sns

cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(6, 5))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=cg_class_names,
    yticklabels=cg_class_names
)
plt.xlabel("Predicted Label")
plt.ylabel("True Label")
plt.title("Confusion Matrix â€“ Coarse-Grained Classification")
plt.tight_layout()
plt.show()